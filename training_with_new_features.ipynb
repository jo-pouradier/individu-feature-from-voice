{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tensorflow\n",
    "!pip install tensorboard"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:17.668333Z",
     "start_time": "2024-12-08T13:24:17.665374Z"
    }
   },
   "source": [
    "data_path = r\"D:\\datasets\\cv-corpus-19.0-2024-09-13-fr\\cv-corpus-19.0-2024-09-13\\fr\"\n",
    "data_path = r\"/mnt/d/datasets/cv-corpus-19.0-2024-09-13-fr/cv-corpus-19.0-2024-09-13/fr\"\n",
    "\n",
    "train_age_only = True\n",
    "use_ordinal_age = False\n",
    "use_early_stopping = False\n",
    "features_length = 40\n",
    "\n",
    "# Hyperparameters\n",
    "loss_age_weight = 1\n",
    "loss_genre_weight = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_age_classes = 7\n",
    "train_ratio = 0.90"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:18.147577Z",
     "start_time": "2024-12-08T13:24:18.143768Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def preprocess_label_data(label_data, keep_age=False):\n",
    "    gender_dict = {\n",
    "        'male_masculine': 0,\n",
    "        'female_feminine': 1\n",
    "    }\n",
    "    age_dict = {\n",
    "        'teens': 0,\n",
    "        'twenties': 1,\n",
    "        'thirties': 2,\n",
    "        'fourties': 3,\n",
    "        'fifties': 4,\n",
    "        'sixties': 5,\n",
    "        'seventies': 6\n",
    "    }\n",
    "    label_data = label_data.copy()\n",
    "    if keep_age:\n",
    "        serie_age = label_data['age'].copy()\n",
    "    if use_ordinal_age:\n",
    "        label_data['age'] = label_data['age'].map(age_dict)\n",
    "    else:\n",
    "        label_data = pd.get_dummies(label_data, columns=['age'])\n",
    "    label_data['gender'] = label_data['gender'].map(gender_dict)\n",
    "    return pd.concat([label_data, serie_age], axis=1) if keep_age else label_data"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:18.725767Z",
     "start_time": "2024-12-08T13:24:18.719281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load CSV data into a DataFrame with specific columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dataset(csv_path, batch_size, num_age_classes, train_ratio=0.8, random_state=0):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    # Load CSV and preprocess\n",
    "    df = load_csv_data(csv_path)\n",
    "    df = preprocess_label_data(df)\n",
    "\n",
    "    # Split the data\n",
    "    train_data, val_data = train_test_split(df, train_size=train_ratio, random_state=random_state,\n",
    "                                            stratify=df['gender'])\n",
    "    print(f\"Train data: {len(train_data)} samples\")\n",
    "    print(f\"Validation data: {len(val_data)} samples\")\n",
    "    # Convert columns to tensors\n",
    "    def convert_to_tensors(data):\n",
    "        features = tf.convert_to_tensor(data['mfcc_features'].values, dtype=tf.float32)\n",
    "        ages = tf.convert_to_tensor(data['age'] if use_ordinal_age else data.iloc[:, 3:].values,\n",
    "                                    dtype=tf.float32)  # Assuming age columns start from index 3\n",
    "        if not train_age_only:\n",
    "            genders = tf.convert_to_tensor(data['gender'].values, dtype=tf.int32)\n",
    "            return features, genders, ages\n",
    "        else:\n",
    "            return features, ages\n",
    "\n",
    "    if not train_age_only:\n",
    "        train_features, train_genders, train_ages = convert_to_tensors(train_data)\n",
    "        val_features, val_genders, val_ages = convert_to_tensors(val_data)\n",
    "        # Create datasets from tensors\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_genders, train_ages))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_genders, val_ages))\n",
    "\n",
    "        # Parse rows, and batch\n",
    "        train_dataset = (\n",
    "            train_dataset\n",
    "            .map(lambda path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "\n",
    "        val_dataset = (\n",
    "            val_dataset\n",
    "            .map(lambda path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "    else:\n",
    "        train_features, train_ages = convert_to_tensors(train_data)\n",
    "        val_features, val_ages = convert_to_tensors(val_data)\n",
    "        # Create datasets from tensors\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_ages))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_ages))\n",
    "\n",
    "        # Parse rows, and batch\n",
    "        train_dataset = (\n",
    "            train_dataset\n",
    "            .map(lambda path, age: tf_parse_row(path, None, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "\n",
    "        val_dataset = (\n",
    "            val_dataset\n",
    "            .map(lambda path, age: tf_parse_row(path, None, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def tf_parse_row(features, gender_label, age_label, num_age_classes):\n",
    "    \"\"\"\n",
    "    Wrapper to use the parse_row function with TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set shapes for TensorFlow to understand\n",
    "    features.set_shape([features_length])  # Ajustez cette forme en fonction de vos données\n",
    "    age_label.set_shape([] if use_ordinal_age else [num_age_classes])\n",
    "    labels = {\"age\": age_label}\n",
    "    if not train_age_only:\n",
    "        gender_label.set_shape([])\n",
    "        labels['gender'] = gender_label\n",
    "    return features, labels\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:19.570383Z",
     "start_time": "2024-12-08T13:24:19.521065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_path = f\"features.csv\"\n",
    "train_dataset, val_dataset = create_dataset(csv_path, batch_size, num_age_classes, train_ratio)\n",
    "train_dataset.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1260 samples\n",
      "Validation data: 140 samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m train_dataset, val_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_age_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_ratio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m train_dataset\u001B[38;5;241m.\u001B[39mhead()\n",
      "Cell \u001B[0;32mIn[11], line 61\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[0;34m(csv_path, batch_size, num_age_classes, train_ratio, random_state)\u001B[0m\n\u001B[1;32m     53\u001B[0m     val_dataset \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     54\u001B[0m         val_dataset\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;241m.\u001B[39mprefetch(tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mAUTOTUNE)  \u001B[38;5;66;03m# Prefetch for efficient data loading\u001B[39;00m\n\u001B[1;32m     59\u001B[0m     )\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     train_features, train_ages \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m     val_features, val_ages \u001B[38;5;241m=\u001B[39m convert_to_tensors(val_data)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# Create datasets from tensors\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[11], line 28\u001B[0m, in \u001B[0;36mcreate_dataset.<locals>.convert_to_tensors\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_to_tensors\u001B[39m(data):\n\u001B[0;32m---> 28\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241m.\u001B[39mconvert_to_tensor(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmfcc_features\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     29\u001B[0m     ages \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m use_ordinal_age \u001B[38;5;28;01melse\u001B[39;00m data\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m3\u001B[39m:]\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[1;32m     30\u001B[0m                                 dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# Assuming age columns start from index 3\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m train_age_only:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:30.267803Z",
     "start_time": "2024-12-08T13:24:27.150506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "\n",
    "\n",
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=(1, 1), padding='same', activation='relu',\n",
    "                 kernel_initializer='he_normal', batch_norm=True, max_pool=True, dropout_rate=0.0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            kernel_initializer=kernel_initializer\n",
    "        )\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            kernel_initializer=kernel_initializer\n",
    "        )\n",
    "        self.batch_norm = layers.BatchNormalization() if batch_norm else None\n",
    "        self.activation = layers.Activation(activation)\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same') if max_pool else None\n",
    "        self.dropout = layers.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs, training=training)\n",
    "        x = self.conv2(x, training=training)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x, training=training)\n",
    "        x = self.activation(x)\n",
    "        if self.max_pool:\n",
    "            x = self.max_pool(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioAgeAndGenderClassifier(tf.keras.Model):\n",
    "    def __init__(self, input_shape=40):\n",
    "        super(AudioAgeAndGenderClassifier, self).__init__()\n",
    "        self.blocks = [\n",
    "            ConvBlock(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            # ConvBlock(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "            #           kernel_initializer='he_normal', batch_norm=True, max_pool=True, dropout_rate=0.5),\n",
    "        ]\n",
    "\n",
    "        self.reduction_layer = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='sigmoid')\n",
    "        self.age_output = layers.Dense(1 if use_ordinal_age else num_age_classes,\n",
    "                                       activation=\"relu\" if use_ordinal_age else 'softmax')\n",
    "        if not train_age_only:\n",
    "            self.gender_output = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.expand_dims(inputs, axis=-1)\n",
    "        # x = inputs\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.reduction_layer(x)\n",
    "        x = self.dense(x, training=training)\n",
    "        age_pred = self.age_output(x)\n",
    "\n",
    "        if not train_age_only:\n",
    "            gender_pred = self.gender_output(x)\n",
    "            return {\"age\": age_pred, \"gender\": gender_pred}\n",
    "        else:\n",
    "            return {\"age\": age_pred}\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"Using GPU: {device_name}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 14:24:27.502677: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-08 14:24:27.647941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733664267.719059    4996 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733664267.734898    4996 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 14:24:27.890660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733664270.265171    4996 gpu_device.cc:2022] Created device /device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T13:24:55.600415Z",
     "start_time": "2024-12-08T13:24:55.526002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def age_loss_function(y_true, y_pred):\n",
    "    # return tf.reduce_mean(tf.square(y_true / num_age_classes - 1 - y_pred))\n",
    "    return tf.abs(tf.reduce_mean(y_true / num_age_classes - 1 - y_pred))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "def compile_model(model, learning_rate=0.001):\n",
    "    # Define separate losses for age and gender\n",
    "    losses = {\n",
    "        \"age\": age_loss_function if use_ordinal_age else tf.keras.losses.CategoricalCrossentropy(),\n",
    "    }\n",
    "    metrics = {\n",
    "        \"age\": [tf.keras.metrics.MeanSquaredError() if use_ordinal_age else tf.keras.metrics.CategoricalAccuracy()],\n",
    "    }\n",
    "    if not train_age_only:\n",
    "        losses[\"gender\"] = tf.keras.losses.BinaryCrossentropy()\n",
    "        metrics[\"gender\"] = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=losses,\n",
    "        loss_weights=[loss_age_weight, loss_genre_weight] if not train_age_only else [loss_age_weight],\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test_classes, y_pred_classes, display_labels, title=\"Confusion Matrix\"):\n",
    "    if not use_ordinal_age:\n",
    "        y_test_classes = np.argmax(y_test_classes, axis=1)\n",
    "        y_pred_classes = np.argmax(y_pred_classes, axis=1)\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "    # Optionally normalize the confusion matrix\n",
    "    # cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "\n",
    "    # Plot the matrix\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_example_output(model, dataset):\n",
    "    predictions_list = []\n",
    "    labels_list = []\n",
    "    for batch in dataset:\n",
    "        features, labels = batch\n",
    "        # Plot confusion matrix using pyplot\n",
    "        predictions = model.predict(features)\n",
    "        predictions_list.append(predictions)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    labels = {key: np.concatenate([label[key] for label in labels_list]) for key in\n",
    "              labels_list[0].keys()}\n",
    "    predictions = {key: np.concatenate([pred[key] for pred in predictions_list]) for key in\n",
    "                   predictions_list[0].keys()}\n",
    "    plot_confusion_matrix(labels[\"age\"], predictions[\"age\"], np.arange(0, np.max(predictions[\"age\"])),\n",
    "                          title=\"Age Confusion Matrix\")\n",
    "    if not train_age_only:\n",
    "        plot_confusion_matrix(labels[\"gender\"], predictions[\"gender\"], ['male', 'female'],\n",
    "                              title=\"Gender Confusion Matrix\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, dataset, validation_dataset, epochs, batch_size, verbose=0):\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=\"./logs\"),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print_example_output(model, val_dataset)\n",
    "        ),\n",
    "    ]\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=3\n",
    "            ), )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "csv_path = f\"{data_path}/validated_filtered_100_per_age.csv\"\n",
    "\n",
    "# csv_path = f\"{data_path}/validated_filtered_100.csv\"\n",
    "csv_path = f\"{data_path}/features.csv\"\n",
    "\n",
    "train_dataset, val_dataset = create_dataset(csv_path, batch_size, num_age_classes, train_ratio)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1260 samples\n",
      "Validation data: 140 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733664295.556381    4996 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[-8.61413086e+02  1.00171486e+02  9.73597908e+00  2.33919487e+01\\n  8.27231824e-01 -6.01615810e+00  9.12188911e+00  1.22771430e+00\\n -1.43263597e+01  8.49530995e-01 -1.41447961e+00 -5.88370609e+00\\n  1.08055580e+00 -3.05928993e+00 -3.21787405e+00 -2.58111286e+00\\n -2.73121309e+00 -1.73651266e+00 -7.71248221e-01 -2.65680814e+00\\n  5.96841972e-04 -3.66110420e+00 -3.75554776e+00  5.14003932e-01\\n -2.37352872e+00 -3.40645432e+00 -3.78667355e+00 -2.99894595e+00\\n -1.76282811e+00  5.32053888e-01 -2.00210023e+00 -4.57469797e+00\\n -3.62796879e+00 -1.65720928e+00 -8.71657014e-01 -2.15640831e+00\\n -2.68343329e+00 -2.19119644e+00 -1.06005609e+00 -1.68563294e+00]'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 108\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# csv_path = f\"{data_path}/validated_filtered_100.csv\"\u001B[39;00m\n\u001B[1;32m    106\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/features.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 108\u001B[0m train_dataset, val_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_age_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_ratio\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 61\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[0;34m(csv_path, batch_size, num_age_classes, train_ratio, random_state)\u001B[0m\n\u001B[1;32m     53\u001B[0m     val_dataset \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     54\u001B[0m         val_dataset\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;241m.\u001B[39mprefetch(tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mAUTOTUNE)  \u001B[38;5;66;03m# Prefetch for efficient data loading\u001B[39;00m\n\u001B[1;32m     59\u001B[0m     )\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     train_features, train_ages \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m     val_features, val_ages \u001B[38;5;241m=\u001B[39m convert_to_tensors(val_data)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# Create datasets from tensors\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[11], line 28\u001B[0m, in \u001B[0;36mcreate_dataset.<locals>.convert_to_tensors\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_to_tensors\u001B[39m(data):\n\u001B[0;32m---> 28\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmfcc_features\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     ages \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m use_ordinal_age \u001B[38;5;28;01melse\u001B[39;00m data\u001B[38;5;241m.\u001B[39miloc[:, \u001B[38;5;241m3\u001B[39m:]\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[1;32m     30\u001B[0m                                 dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# Assuming age columns start from index 3\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m train_age_only:\n",
      "File \u001B[0;32m~/.virtualenvs/IA-DeepLearning-5ETI/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.virtualenvs/IA-DeepLearning-5ETI/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    106\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    107\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: '[-8.61413086e+02  1.00171486e+02  9.73597908e+00  2.33919487e+01\\n  8.27231824e-01 -6.01615810e+00  9.12188911e+00  1.22771430e+00\\n -1.43263597e+01  8.49530995e-01 -1.41447961e+00 -5.88370609e+00\\n  1.08055580e+00 -3.05928993e+00 -3.21787405e+00 -2.58111286e+00\\n -2.73121309e+00 -1.73651266e+00 -7.71248221e-01 -2.65680814e+00\\n  5.96841972e-04 -3.66110420e+00 -3.75554776e+00  5.14003932e-01\\n -2.37352872e+00 -3.40645432e+00 -3.78667355e+00 -2.99894595e+00\\n -1.76282811e+00  5.32053888e-01 -2.00210023e+00 -4.57469797e+00\\n -3.62796879e+00 -1.65720928e+00 -8.71657014e-01 -2.15640831e+00\\n -2.68343329e+00 -2.19119644e+00 -1.06005609e+00 -1.68563294e+00]'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-05T15:07:21.457076300Z",
     "start_time": "2024-12-05T15:07:06.460448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "model = AudioAgeAndGenderClassifier(input_shape=features_length)\n",
    "model = compile_model(model, learning_rate)\n",
    "history = train_model(model, train_dataset, val_dataset, epochs, batch_size, verbose=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m20/20\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 834ms/step - categorical_accuracy: 0.1227 - loss: 5.1091"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:59:24.417622Z",
     "start_time": "2024-12-05T14:59:24.400995Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"audio_age_and_gender_classifier_11\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"audio_age_and_gender_classifier_11\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_block_78 (\u001B[38;5;33mConvBlock\u001B[0m)       │ ?                      │           \u001B[38;5;34m224\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001B[38;5;33mFlatten\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m441344\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │    \u001B[38;5;34m56,492,160\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m129\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_block_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)       │ ?                      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">441344</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">56,492,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m169,477,477\u001B[0m (646.51 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">169,477,477</span> (646.51 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m56,492,481\u001B[0m (215.50 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,492,481</span> (215.50 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m32\u001B[0m (128.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> (128.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m112,984,964\u001B[0m (431.00 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">112,984,964</span> (431.00 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:37:42.445537Z",
     "start_time": "2024-12-06T15:37:42.140068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualise a batch on the same figure\n",
    "import matplotlib.pyplot as plt\n",
    "features, labels = train_dataset.take(1).as_numpy_iterator().next()\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(features):\n",
    "    if i != 5:\n",
    "        continue\n",
    "    print(f\"Feature shape: {feature.shape}\")\n",
    "    plt.imshow(feature, cmap='viridis')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (128, 862)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
