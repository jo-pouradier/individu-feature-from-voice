{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction d'age et de sexe à partir de voix humaine\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans ce notebook, nous allons essayer de prédire l'âge et le sexe d'une personne à partir de sa voix. Pour cela, nous allons utiliser un ensemble de données de voix humaine disponible sur [Commonvoice](https://commonvoice.mozilla.org/fr/datasets ). Cet ensemble de données contient des enregistrements vocaux de personnes, chacun étant associé à un label indiquant le sexe de la personne (homme ou femme) et son âge. Nous allons entraîner un modèle de deeplearning sur ces données pour prédire le sexe et l'âge d'une personne à partir de sa voix.\n",
    "\n",
    "1. [Paramètres](#Paramètres)\n",
    "2. [Préparation des données](#Preparation-des-donnees)\n",
    "3. [Traitements des audios](#Traitements-des-audios)\n",
    "4. [Création du modèle](#Création-du-modèle)\n",
    "5. [Entrainer le modèle](#4)\n",
    "6. [Résultats](#5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres\n",
    "\n",
    "On a une liste de paramètres qui vont nous permettre de configurer le modèle, les données et l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_age_only = True\n",
    "use_ordinal_age = False\n",
    "use_early_stopping = False\n",
    "features_length = 40\n",
    "\n",
    "# Hyperparameters\n",
    "loss_age_weight = 1\n",
    "loss_genre_weight = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_age_classes = 7\n",
    "train_ratio = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "Le dataset choisis comportait énormément de lignes nous avons donc décidé dans un premier temps de supprimer toutes les lignes ou une valeur manquait. Puisque nous voulons prédire l'âge et le sexe nous avons ensuite trier le dataset pour voir la proportion de chaque sexe par tranche d'âge. A partir de ce résultat nous avons décidé de garder 10_000 lignes pour chaque catégorie d'âge avec une répartition égale d'hommes et de femmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"D:\\datasets\\cv-corpus-19.0-2024-09-13-fr\\cv-corpus-19.0-2024-09-13\\fr\"\n",
    "data_path = r\"/mnt/d/datasets/cv-corpus-19.0-2024-09-13-fr/cv-corpus-19.0-2024-09-13/fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\datasets\\\\cv-corpus-19.0-2024-09-13-fr\\\\cv-corpus-19.0-2024-09-13\\\\fr/validated.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcv-corpus-19.0-2024-09-13-fr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcv-corpus-19.0-2024-09-13\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/validated.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m keep_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m,]\n\u001b[0;32m      7\u001b[0m export_df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()[keep_columns]\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1405\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1392\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1393\u001b[0m     dialect,\n\u001b[0;32m   1394\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1402\u001b[0m )\n\u001b[0;32m   1403\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\datasets\\\\cv-corpus-19.0-2024-09-13-fr\\\\cv-corpus-19.0-2024-09-13\\\\fr/validated.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table(f\"{data_path}/validated.tsv\")\n",
    "\n",
    "keep_columns = ['path', 'sentence','age', 'gender',]\n",
    "export_df = data.copy()[keep_columns]\n",
    "\n",
    "#filter out the rows with missing values\n",
    "import math\n",
    "print(export_df.isnull().sum()/len(export_df)*100)\n",
    "print(\"with na : \", export_df.shape)\n",
    "print(\"without na : \",export_df.dropna().shape)\n",
    "print(f\"percent of data lost : {math.floor((export_df.shape[0] - export_df.dropna().shape[0])/export_df.shape[0]*100)}%\")\n",
    "filtered_df = export_df.copy().dropna()\n",
    "filtered_df = filtered_df[filtered_df['gender'] != \"non-binary\"]\n",
    "filtered_df = filtered_df[filtered_df['gender'] != \"do_not_wish_to_say\"]\n",
    "\n",
    "\n",
    "# plot the distribution of age by gender\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def show_distrib(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.histplot(data=df, x='age', hue='gender', multiple='stack', stat='count')\n",
    "\n",
    "    # Annotate the bars with their counts\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    plt.title('Distribution of Age by Gender')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "show_distrib(filtered_df)\n",
    "\n",
    "\n",
    "# export a csv with X rows by age category with the same distribution of men and woman\n",
    "age_df_list = []\n",
    "for age in filtered_df['age'].unique():\n",
    "    df_age = filtered_df[filtered_df['age'] == age]\n",
    "    df_male = df_age[df_age['gender'] == 'male_masculine']\n",
    "    df_female = df_age[df_age['gender'] == 'female_feminine']\n",
    "    count_per_gender = min(row_per_age, len(df_female))\n",
    "    df_tmp = pd.concat([df_female[:count_per_gender], df_male[:count_per_gender]])\n",
    "    age_df_list.append(df_tmp)\n",
    "export_df = pd.concat(age_df_list)\n",
    "export_df.head()\n",
    "\n",
    "show_distrib(export_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitements des audios\n",
    "\n",
    "Nous étions d'abord parti pour utiliser le spectrogramme des audios en normalisant l'audio et en modifiant la durée à 10 secondes. Cependant, les résultats obtenus n'étaient pas pertinent (~0.23 d'accuracy) nous avons donc décidé de changer de méthode. Nous avons utilisé la librairie librosa pour transformer les audios en MFCC (Mel-Frequency Cepstral Coefficients) qui sont des caractéristiques audio qui représentent le spectre de puissance d'un son, utilisées pour l'analyse et la reconnaissance vocale. Nous avons aussi fait la moyenne des MFCC pour chaque audio pour réduire la dimension des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_wav(file_path: str):\n",
    "    \"\"\"\n",
    "    Load a wav file using librosa\n",
    "    :param file_path: Path to the wav file\n",
    "    :return: Tuple of the audio data and the sample rate\n",
    "    \"\"\"\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    return audio_data, sample_rate\n",
    "\n",
    "\n",
    "def get_spectrogram(audio):\n",
    "    # Short-time Fourier transform (STFT).\n",
    "    stft = abs(librosa.stft(audio))\n",
    "\n",
    "    # Convert an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    spectrogram = librosa.amplitude_to_db(stft)\n",
    "\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def get_mfcc(audio, sampling_rate, n_mfcc=20):\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def clip_audio(audio, duration=10, repeat=False):\n",
    "    \"\"\"\n",
    "    Clip the audio to a specific duration\n",
    "    :param audio: Audio data\n",
    "    :param duration: Duration to clip the audio to\n",
    "    :return: Clipped audio data\n",
    "    \"\"\"\n",
    "    # Get the number of samples for the desired duration\n",
    "    n_samples = int(duration * sample_rate)\n",
    "    if len(audio) < n_samples:\n",
    "        # Repeat the audio if it is shorter than the desired duration\n",
    "        if repeat:\n",
    "            n_repeats = n_samples // len(audio)\n",
    "            n_samples = n_repeats * len(audio)\n",
    "            audio = np.tile(audio, n_repeats)\n",
    "        else:\n",
    "            # Pad the audio with zeros if it is shorter than the desired duration\n",
    "            n_zeros = n_samples - len(audio)\n",
    "            audio = np.pad(audio, (0, n_zeros))\n",
    "    # Clip the audio\n",
    "    clipped_audio = audio[:n_samples]\n",
    "    return clipped_audio\n",
    "\n",
    "\n",
    "def extract_features(file_name, base_path, output_path, save=False):\n",
    "    audio_data, sample_rate = load_wav(os.path.join(base_path, file_name))\n",
    "    spectrogram = get_mfcc(audio_data, sample_rate, n_mfcc=40)\n",
    "    features = []\n",
    "    for el in spectrogram:\n",
    "        features.append(np.mean(el))\n",
    "    features = np.array(features)\n",
    "    if save:\n",
    "        np.save(os.path.join(output_path, file_name.split(\".\")[0]), features)\n",
    "    return file_name, features\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # apply_async example\n",
    "    base_path = r\"/mnt/d/datasets/cv-corpus-19.0-2024-09-13-fr/cv-corpus-19.0-2024-09-13/fr/clips/\"\n",
    "    start_time = time.time()\n",
    "    num_processes = 8\n",
    "    data = pd.read_csv(os.path.join(\"../\",\"validated_filtered_500_per_age.csv\"), sep=',')\n",
    "    paths = data['path'].to_list()\n",
    "    # paths = paths[:8]\n",
    "    print(f\"Number of paths: {len(paths)}\")\n",
    "    with Pool(num_processes) as p:\n",
    "        async_result = p.starmap_async(extract_features, [(path, base_path, \"processed\") for path in paths])\n",
    "        print('Waiting for results...')\n",
    "        results = async_result.get()\n",
    "    print(f\"Multi --- {time.time() - start_time} seconds ---\")\n",
    "    df = pd.DataFrame(results, columns=[\"path\", \"mfcc_features\"])\n",
    "    # merge the features with the data\n",
    "    data = data.merge(df, left_on='path', right_on='path')\n",
    "    data.to_csv(\"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset\n",
    "\n",
    "Nous avons créé un dataset avec les MFCC moyens de chaque audio et les labels associés. Nous avons ensuite divisé ce dataset en un ensemble d'entraînement et un ensemble de test. On utilise Tensorflow pour charger les données et les préparer pour l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour encoder le genre nous avons choisi un encodage binaire 0 (male) et 1 (femelle).\n",
    "Pour l'âge nous avons essayé plusieurs encodages : \n",
    "- l'encodage ordinnal semble être une option logique pour l'âge, mais les résultats n'étaient pas ceux espérés .\n",
    "- Le hot-one encoding a donné de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def preprocess_label_data(label_data, keep_age=False):\n",
    "    gender_dict = {\n",
    "        'male_masculine': 0,\n",
    "        'female_feminine': 1\n",
    "    }\n",
    "    age_dict = {\n",
    "        'teens': 0,\n",
    "        'twenties': 1,\n",
    "        'thirties': 2,\n",
    "        'fourties': 3,\n",
    "        'fifties': 4,\n",
    "        'sixties': 5,\n",
    "        'seventies': 6\n",
    "    }\n",
    "    label_data = label_data.copy()\n",
    "    mfcc_features = []\n",
    "    for i in range(features_length):\n",
    "        mfcc_features.append(label_data[f'mfcc_{i}'])\n",
    "        label_data = label_data.drop(f'mfcc_{i}', axis=1)\n",
    "    label_data['mfcc_features'] = list(np.asarray(mfcc_features).T)\n",
    "    if keep_age:\n",
    "        serie_age = label_data['age'].copy()\n",
    "    if use_ordinal_age:\n",
    "        label_data['age'] = label_data['age'].map(age_dict)\n",
    "    else:\n",
    "        label_data = pd.get_dummies(label_data, columns=['age'])\n",
    "    label_data['gender'] = label_data['gender'].map(gender_dict)\n",
    "    return pd.concat([label_data, serie_age], axis=1) if keep_age else label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la création du datasets nous avons plusieurs étapes :\n",
    "- Charger les données.\n",
    "- Encoder les labels.\n",
    "- Diviser les données en un ensemble d'entraînement et un ensemble de test.\n",
    "- Créer des datasets TensorFlow à partir des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load CSV data into a DataFrame with specific columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dataset(csv_path, batch_size, num_age_classes, train_ratio=0.8, random_state=0):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    # Load CSV and preprocess\n",
    "    df = load_csv_data(csv_path)\n",
    "    df = preprocess_label_data(df)\n",
    "\n",
    "    # Split the data\n",
    "    train_data, val_data = train_test_split(df, train_size=train_ratio, random_state=random_state,\n",
    "                                            stratify=df['gender'])\n",
    "    print(f\"Train data: {len(train_data)} samples\")\n",
    "    print(f\"Validation data: {len(val_data)} samples\")\n",
    "    # Convert columns to tensors\n",
    "    def convert_to_tensors(data):\n",
    "        features = data['mfcc_features']\n",
    "        features = tf.convert_to_tensor(np.stack(features), dtype=tf.float32)\n",
    "        ages = tf.convert_to_tensor(data['age'] if use_ordinal_age else data.iloc[:, 4:].values,\n",
    "                                    dtype=tf.float32)  # Assuming age columns start from index 3\n",
    "        if not train_age_only:\n",
    "            genders = tf.convert_to_tensor(data['gender'].values, dtype=tf.int32)\n",
    "            return features, genders, ages\n",
    "        else:\n",
    "            return features, ages\n",
    "\n",
    "    if not train_age_only:\n",
    "        train_features, train_genders, train_ages = convert_to_tensors(train_data)\n",
    "        val_features, val_genders, val_ages = convert_to_tensors(val_data)\n",
    "        # Create datasets from tensors\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_genders, train_ages))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_genders, val_ages))\n",
    "\n",
    "        # Parse rows, and batch\n",
    "        train_dataset = (\n",
    "            train_dataset\n",
    "            .map(lambda path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "\n",
    "        val_dataset = (\n",
    "            val_dataset\n",
    "            .map(lambda path, gender, age: tf_parse_row(path, gender, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "    else:\n",
    "        train_features, train_ages = convert_to_tensors(train_data)\n",
    "        val_features, val_ages = convert_to_tensors(val_data)\n",
    "        # Create datasets from tensors\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_ages))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_ages))\n",
    "\n",
    "        # Parse rows, and batch\n",
    "        train_dataset = (\n",
    "            train_dataset\n",
    "            .map(lambda features, age: tf_parse_row(features, None, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "\n",
    "        val_dataset = (\n",
    "            val_dataset\n",
    "            .map(lambda features, age: tf_parse_row(features, None, age, num_age_classes),\n",
    "                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(batch_size)\n",
    "            .prefetch(tf.data.AUTOTUNE)  # Prefetch for efficient data loading\n",
    "        )\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def tf_parse_row(features, gender_label, age_label, num_age_classes):\n",
    "    \"\"\"\n",
    "    Wrapper to use the parse_row function with TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set shapes for TensorFlow to understand\n",
    "    features.set_shape([features_length])  # Ajustez cette forme en fonction de vos données\n",
    "    age_label.set_shape([] if use_ordinal_age else [num_age_classes])\n",
    "    labels = {\"age\": age_label}\n",
    "    if not train_age_only:\n",
    "        gender_label.set_shape([])\n",
    "        labels['gender'] = gender_label\n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle\n",
    "\n",
    "Nous avons voulu reconnaitre des patternes dans les audios pour cela on a choisi des couches de convolution qui sont très utilisées pour la reconnaissance de patternes. Nous avons combiné ces couches avec des couches de maxpooling pour réduire la dimension des données et des couches de dropout pour éviter l'overfitting. \n",
    "Des couches de dense ont été ajoutées pour la classification des différents patterns reconnus. \n",
    "\n",
    "Pour les fonctions de pertes nous avons utilisé la Binary Crossentropy pour le sexe et la Categorical Crossentropy pour l'age. Nous avons aussi jouer sur le poid de chaque fonction de perte pour guider le modèle dans son apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, initializers\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConvBlock\u001b[39;00m(layers\u001b[38;5;241m.\u001b[39mLayer):\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_model_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_proto\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_saveable_from_iterator\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInitializer\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_table_from_dataset\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_from_dataset\n",
      "File \u001b[1;32md:\\VS CODE\\individu-feature-from-voice\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\lookup_ops.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1131\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "\n",
    "\n",
    "class ConvBlock(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=(1, 1), padding='same', activation='relu',\n",
    "                 kernel_initializer='he_normal', batch_norm=True, max_pool=True, dropout_rate=0.0):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            kernel_initializer=kernel_initializer\n",
    "        )\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            kernel_initializer=kernel_initializer\n",
    "        )\n",
    "        self.batch_norm = layers.BatchNormalization() if batch_norm else None\n",
    "        self.activation = layers.Activation(activation)\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same') if max_pool else None\n",
    "        self.dropout = layers.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs, training=training)\n",
    "        x = self.conv2(x, training=training)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x, training=training)\n",
    "        x = self.activation(x)\n",
    "        if self.max_pool:\n",
    "            x = self.max_pool(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioAgeAndGenderClassifier(tf.keras.Model):\n",
    "    def __init__(self, input_shape=40):\n",
    "        super(AudioAgeAndGenderClassifier, self).__init__()\n",
    "        self.blocks = [\n",
    "            ConvBlock(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            ConvBlock(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer='he_normal', batch_norm=False, max_pool=True, dropout_rate=0.0),\n",
    "            # ConvBlock(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu',\n",
    "            #           kernel_initializer='he_normal', batch_norm=True, max_pool=True, dropout_rate=0.5),\n",
    "        ]\n",
    "\n",
    "        self.reduction_layer = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='sigmoid')\n",
    "        self.age_output = layers.Dense(1 if use_ordinal_age else num_age_classes,\n",
    "                                       activation=\"relu\" if use_ordinal_age else 'softmax')\n",
    "        if not train_age_only:\n",
    "            self.gender_output = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.expand_dims(inputs, axis=-1)\n",
    "        # x = inputs\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        x = self.reduction_layer(x)\n",
    "        x = self.dense(x, training=training)\n",
    "        age_pred = self.age_output(x)\n",
    "\n",
    "        if not train_age_only:\n",
    "            gender_pred = self.gender_output(x)\n",
    "            return {\"age\": age_pred, \"gender\": gender_pred}\n",
    "        else:\n",
    "            return {\"age\": age_pred}\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name:\n",
    "    print(f\"Using GPU: {device_name}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainer le modèle\n",
    "\n",
    "Une fois le modèle définit on peut l'entrainer. Nous avons fait  plusieurs entrainments en faisant varier les paramètres pour comparer l'influence de ces derniers sur les résultats. Pour pouvoir finalement trouver les meilleurs paramètres pour notre modèle.\n",
    "\n",
    "L'âge étant un label plus complexe à prédire que le genre on peut choisir d'entrainer le modèle que sur l'âge pour mieux régler les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, dataset, validation_dataset, epochs, batch_size, verbose=0):\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=\"./logs\"),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print_example_output(model, val_dataset)\n",
    "        ),\n",
    "    ]\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=3\n",
    "            ), )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "csv_path = f\"{data_path}/validated_filtered_100_per_age.csv\"\n",
    "\n",
    "# csv_path = f\"{data_path}/validated_filtered_100.csv\"\n",
    "csv_path = f\"{data_path}/features.csv\"\n",
    "\n",
    "train_dataset, val_dataset = create_dataset(csv_path, batch_size, num_age_classes, train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats\n",
    "\n",
    "#TODO penser à mettre en gras les meilleurs résultats de chaques colonnes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
